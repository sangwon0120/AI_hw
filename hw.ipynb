{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a14b46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# E-step: seed weight 최적화(단순 simplex grid 0.05 간격)\n",
    "#  - winsor=1.6, BINS=24, DROPOUT=0.06, TTA=14, PATIENCE=18\n",
    "#  - Ridge(α inner-tune [2.6,2.8,3.0,3.2,3.4]) + Residual MLP + EMA\n",
    "#  - fold trimmed mean(20%) → seed-weight grid search → 최종 예측\n",
    "#  - Saves & downloads: /content/submission_YJ_Estep.csv\n",
    "# ============================================================\n",
    "\n",
    "import os, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ---------------- config ----------------\n",
    "DATA_DIR = \"/content/drive/MyDrive/인공지능 과제2\"\n",
    "SAVE_DIR = \"/content\"\n",
    "SEEDS    = (42, 2025, 7, 11, 77)\n",
    "\n",
    "BINS     = 24\n",
    "INNER_ALPHAS = [2.6, 2.8, 3.0, 3.2, 3.4]\n",
    "\n",
    "WINSOR  = 1.6\n",
    "DROPOUT = 0.06\n",
    "BETA    = 1.25\n",
    "EMA_DEC = 0.996\n",
    "TTA     = 14\n",
    "BATCH   = 96\n",
    "EPOCHS  = 220\n",
    "PATIENCE= 18\n",
    "TRIM_RATIO = 0.20\n",
    "SOFT_CLIP = True\n",
    "\n",
    "# ---------------- utils ----------------\n",
    "def seed_all(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "def rmse(y, p):\n",
    "    y = np.asarray(y, dtype=np.float64).reshape(-1)\n",
    "    p = np.asarray(p, dtype=np.float64).reshape(-1)\n",
    "    return float(np.sqrt(np.mean((y - p) ** 2)))\n",
    "\n",
    "def stratified_kfold_regression(y, n_splits=5, seed=42, n_bins=24):\n",
    "    y = np.asarray(y).reshape(-1)\n",
    "    q = int(min(n_bins, max(2, np.unique(y).size)))\n",
    "    y_bins = pd.qcut(pd.Series(y), q=q, duplicates=\"drop\").cat.codes.values\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idx = rng.permutation(len(y))\n",
    "    yb = y_bins[idx]\n",
    "    buckets = [[] for _ in range(n_splits)]\n",
    "    for b in np.unique(yb):\n",
    "        b_idx = idx[yb == b]\n",
    "        for k, j in enumerate(range(len(b_idx))):\n",
    "            buckets[k % n_splits].append(b_idx[j])\n",
    "    folds = []\n",
    "    for k in range(n_splits):\n",
    "        va_idx = np.array(sorted(buckets[k]))\n",
    "        tr_mask = np.ones(len(y), dtype=bool); tr_mask[va_idx] = False\n",
    "        tr_idx = np.where(tr_mask)[0]\n",
    "        folds.append((tr_idx, va_idx))\n",
    "    return folds\n",
    "\n",
    "def interactions_min(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    if \"bw\" in df.columns and \"b.head\" in df.columns:\n",
    "        df[\"bw__x__b.head\"] = df[\"bw\"] * df[\"b.head\"]\n",
    "        df[\"head_bw_ratio\"] = df[\"b.head\"] / (df[\"bw\"] + 1e-6)\n",
    "    if \"preterm\" in df.columns and \"bw\" in df.columns:\n",
    "        df[\"preterm__x__bw\"] = df[\"preterm\"] * df[\"bw\"]\n",
    "    if \"nnhealth\" in df.columns and \"b.head\" in df.columns:\n",
    "        df[\"nnhealth__x__b.head\"] = df[\"nnhealth\"] * df[\"b.head\"]\n",
    "    return df\n",
    "\n",
    "def trimmed_mean(arr, trim_ratio=0.2, axis=0):\n",
    "    arr = np.sort(arr, axis=axis)\n",
    "    n = arr.shape[axis]\n",
    "    k = int(n * trim_ratio)\n",
    "    if n - 2*k <= 0:\n",
    "        return arr.mean(axis=axis)\n",
    "    if axis == 0:\n",
    "        return arr[k:n-k].mean(axis=0)\n",
    "    else:\n",
    "        return arr[:, k:n-k].mean(axis=1)\n",
    "\n",
    "def soft_clip_to_iqr(pred, y_train, verbose=True):\n",
    "    q1, q3 = np.percentile(y_train, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lo = q1 - 1.5 * iqr\n",
    "    hi = q3 + 1.5 * iqr\n",
    "    if verbose:\n",
    "        print(f\"[Clip-range] lo={lo:.3f}, hi={hi:.3f}\")\n",
    "    return np.clip(pred, lo, hi)\n",
    "\n",
    "# ---------------- model ----------------\n",
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, d, sizes=(64,32), dropout=0.10):\n",
    "        super().__init__()\n",
    "        h1, h2 = sizes\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(d, h1), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(h1, h2), nn.ReLU(),\n",
    "            nn.Linear(h2, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.seq(x)\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.996):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k,v in model.state_dict().items()}\n",
    "    def update(self, model):\n",
    "        with torch.no_grad():\n",
    "            for k, v in model.state_dict().items():\n",
    "                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1-self.decay)\n",
    "    def copy_to(self, model): model.load_state_dict(self.shadow)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_mc(model, X, tta=TTA):\n",
    "    model.train()  # dropout on\n",
    "    outs = []\n",
    "    for _ in range(tta):\n",
    "        outs.append(model(X).cpu().numpy().reshape(-1))\n",
    "    return np.mean(np.stack(outs, axis=0), axis=0)\n",
    "\n",
    "# ---------------- one fold train ----------------\n",
    "def train_residual_fold_YJ(\n",
    "    X_tr, y_tr, X_va, y_va, X_te,\n",
    "    inner_alphas=INNER_ALPHAS, beta=BETA, lr=6e-4, weight_decay=1e-4,\n",
    "    batch_size=BATCH, epochs=EPOCHS, patience=PATIENCE, mlp_sizes=(64,32), dropout=DROPOUT,\n",
    "    winsor_pct=WINSOR, ema_decay=EMA_DEC, tta=TTA\n",
    "):\n",
    "    # 0) winsorize\n",
    "    if winsor_pct and winsor_pct > 0:\n",
    "        lo, hi = np.percentile(y_tr, [winsor_pct, 100 - winsor_pct])\n",
    "        y_tr_fit = y_tr.clip(lo, hi)\n",
    "    else:\n",
    "        y_tr_fit = y_tr\n",
    "\n",
    "    # α mini-tune (3-fold)\n",
    "    best_a, best_rm = None, 1e9\n",
    "    kf_in = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    for a in inner_alphas:\n",
    "        scs=[]\n",
    "        for it_tr, it_va in kf_in.split(X_tr):\n",
    "            X_it_tr, X_it_va = X_tr.iloc[it_tr], X_tr.iloc[it_va]\n",
    "            y_it_tr, y_it_va = y_tr_fit.iloc[it_tr], y_tr.iloc[it_va]\n",
    "            yt_pt_inner = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "            y_it_tr_t = yt_pt_inner.fit_transform(y_it_tr.values.reshape(-1,1)).reshape(-1)\n",
    "            pipe = Pipeline([\n",
    "                (\"pt\", PowerTransformer(method=\"yeo-johnson\", standardize=False)),\n",
    "                (\"sc\", StandardScaler()),\n",
    "                (\"rg\", Ridge(alpha=a, random_state=42)),\n",
    "            ])\n",
    "            pipe.fit(X_it_tr, y_it_tr_t)\n",
    "            pred_t = pipe.predict(X_it_va)\n",
    "            pred   = yt_pt_inner.inverse_transform(pred_t.reshape(-1,1)).reshape(-1)\n",
    "            scs.append(rmse(y_it_va, pred))\n",
    "        m = float(np.mean(scs))\n",
    "        if m < best_rm: best_rm, best_a = m, a\n",
    "\n",
    "    # 본학습: Ridge(YJ target) + residual MLP\n",
    "    ridge_pipe = Pipeline([\n",
    "        (\"pt\", PowerTransformer(method=\"yeo-johnson\", standardize=False)),\n",
    "        (\"sc\", StandardScaler()),\n",
    "        (\"rg\", Ridge(alpha=best_a, random_state=42)),\n",
    "    ])\n",
    "    yt_pt = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "    y_tr_t = yt_pt.fit_transform(y_tr_fit.values.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "    ridge_pipe.fit(X_tr, y_tr_t)\n",
    "    yhat_tr_t = ridge_pipe.predict(X_tr)\n",
    "    yhat_va_t = ridge_pipe.predict(X_va)\n",
    "    yhat_te_t = ridge_pipe.predict(X_te)\n",
    "\n",
    "    resid_tr = y_tr_t - yhat_tr_t\n",
    "    Xtr_tf = ridge_pipe.named_steps[\"sc\"].transform(ridge_pipe.named_steps[\"pt\"].transform(X_tr))\n",
    "    Xva_tf = ridge_pipe.named_steps[\"sc\"].transform(ridge_pipe.named_steps[\"pt\"].transform(X_va))\n",
    "    Xte_tf = ridge_pipe.named_steps[\"sc\"].transform(ridge_pipe.named_steps[\"pt\"].transform(X_te))\n",
    "\n",
    "    mu, sd = resid_tr.mean(), resid_tr.std()\n",
    "    rtr_z = (resid_tr - mu) / (sd + 1e-8)\n",
    "\n",
    "    Xtr_t = torch.tensor(Xtr_tf, dtype=torch.float32)\n",
    "    rtr_t = torch.tensor(np.asarray(rtr_z).reshape(-1,1), dtype=torch.float32)\n",
    "    Xva_t = torch.tensor(Xva_tf, dtype=torch.float32)\n",
    "    Xte_t = torch.tensor(Xte_tf, dtype=torch.float32)\n",
    "\n",
    "    model = SmallMLP(d=Xtr_t.shape[1], sizes=mlp_sizes, dropout=dropout)\n",
    "    try:\n",
    "        crit = nn.SmoothL1Loss(beta=beta)\n",
    "    except TypeError:\n",
    "        crit = nn.L1Loss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=10)\n",
    "    ema = EMA(model, decay=ema_decay)\n",
    "\n",
    "    best, wait, best_state = 1e9, 0, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in DataLoader(TensorDataset(Xtr_t, rtr_t), batch_size=batch_size, shuffle=True):\n",
    "            opt.zero_grad(); loss = crit(model(xb), yb); loss.backward(); opt.step()\n",
    "            ema.update(model)\n",
    "        # valid with EMA\n",
    "        ema.copy_to(model); model.eval()\n",
    "        with torch.no_grad():\n",
    "            rz_va = model(Xva_t).cpu().numpy().reshape(-1)\n",
    "        r_va_t = rz_va*(sd+1e-8) + mu\n",
    "        pred_va_t = yhat_va_t + r_va_t\n",
    "        pred_va = yt_pt.inverse_transform(pred_va_t.reshape(-1,1)).reshape(-1)\n",
    "        sc = rmse(y_va, pred_va)\n",
    "        sch.step(sc)\n",
    "        if sc < best - 1e-6:\n",
    "            best = sc; best_state = {k: v.detach().clone() for k,v in model.state_dict().items()}; wait=0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= patience: break\n",
    "\n",
    "    # best → OOF/TEST (MC Dropout)\n",
    "    model.load_state_dict(best_state); ema.copy_to(model)\n",
    "    with torch.no_grad():\n",
    "        rz_va = predict_mc(model, Xva_t, tta=tta)\n",
    "        r_va_t = rz_va*(sd+1e-8) + mu\n",
    "        oof_t  = yhat_va_t + r_va_t\n",
    "        oof    = yt_pt.inverse_transform(oof_t.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "        rz_te = predict_mc(model, Xte_t, tta=tta)\n",
    "        r_te_t = rz_te*(sd+1e-8) + mu\n",
    "        te_t   = yhat_te_t + r_te_t\n",
    "        te     = yt_pt.inverse_transform(te_t.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "    return oof, te, best, best_a\n",
    "\n",
    "def interactions_min_all(train_df, test_df, feat_cols):\n",
    "    X  = interactions_min(train_df[feat_cols].copy())\n",
    "    Xt = interactions_min(test_df[feat_cols].copy())\n",
    "    return X, Xt\n",
    "\n",
    "# ---------------- seed weight grid search ----------------\n",
    "def simplex_weight_grid(n=5, step=0.05):\n",
    "    # 생성: w_i >=0, sum=1\n",
    "    vals = np.arange(0, 1+1e-9, step)\n",
    "    weights = []\n",
    "    def rec(prefix, remain, k):\n",
    "        if k == n-1:\n",
    "            w = prefix + [remain]\n",
    "            if remain >= -1e-9:\n",
    "                weights.append(np.array(w, dtype=np.float64))\n",
    "            return\n",
    "        for v in vals:\n",
    "            if v <= remain + 1e-9:\n",
    "                rec(prefix + [v], remain - v, k+1)\n",
    "    rec([], 1.0, 0)\n",
    "    return weights\n",
    "\n",
    "# ---------------- main ----------------\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# data\n",
    "train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "test  = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "sub   = pd.read_csv(f\"{DATA_DIR}/sample_submission.csv\")\n",
    "if \"dadage\" in train.columns:\n",
    "    train[\"dadage\"] = train[\"dadage\"].fillna(train[\"dadage\"].mean())\n",
    "    test[\"dadage\"]  = test[\"dadage\"].fillna(train[\"dadage\"].mean())\n",
    "\n",
    "FEATS = [c for c in train.columns if c not in [\"id\",\"y\"]]\n",
    "X_raw = train[FEATS].copy(); y = train[\"y\"].copy(); X_te_raw = test[FEATS].copy()\n",
    "X, X_te = interactions_min_all(train, test, FEATS)\n",
    "\n",
    "# seeds×fold\n",
    "seed_preds = []; seed_oofs = []; seed_scores = []; seed_alpha_means = []\n",
    "for s in SEEDS:\n",
    "    seed_all(s)\n",
    "    folds = stratified_kfold_regression(y, n_splits=5, seed=s, n_bins=BINS)\n",
    "    oof = np.zeros(len(X)); te_fold=[]; alphas=[]\n",
    "    for tr_idx, va_idx in folds:\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n",
    "        o, t, sc, a = train_residual_fold_YJ(\n",
    "            X_tr, y_tr, X_va, y_va, X_te,\n",
    "            inner_alphas=INNER_ALPHAS, beta=BETA, dropout=DROPOUT,\n",
    "            winsor_pct=WINSOR, ema_decay=EMA_DEC, tta=TTA,\n",
    "            batch_size=BATCH, epochs=EPOCHS, patience=PATIENCE\n",
    "        )\n",
    "        oof[va_idx]=o; te_fold.append(t); alphas.append(a)\n",
    "    sc_seed = rmse(y, oof)\n",
    "    te_fold = np.vstack(te_fold)\n",
    "    pred_seed = trimmed_mean(te_fold, trim_ratio=TRIM_RATIO, axis=0)\n",
    "    seed_preds.append(pred_seed)\n",
    "    seed_oofs.append(oof)\n",
    "    seed_scores.append(sc_seed)\n",
    "    seed_alpha_means.append(np.mean(alphas))\n",
    "    print(f\"seed {s} OOF={sc_seed:.5f} (α~{np.mean(alphas):.2f})\")\n",
    "print(\"OOF per seed:\", [f\"{v:.5f}\" for v in seed_scores])\n",
    "\n",
    "oof_mat   = np.vstack(seed_oofs)   # (S, N)\n",
    "pred_mat  = np.vstack(seed_preds)  # (S, N)\n",
    "\n",
    "# ---- seed weight grid search (simplex step=0.05) ----\n",
    "cands = simplex_weight_grid(n=len(SEEDS), step=0.05)\n",
    "best = {\"w\": None, \"rmse\": 1e9}\n",
    "y_np = y.values\n",
    "for w in cands:\n",
    "    oof_w  = np.sum(oof_mat.T  * w, axis=1)\n",
    "    sc = rmse(y_np, oof_w)\n",
    "    if sc < best[\"rmse\"]:\n",
    "        best = {\"w\": w, \"rmse\": sc}\n",
    "print(f\"[WeightSearch] best OOF RMSE={best['rmse']:.5f} | w={np.round(best['w'],3)}\")\n",
    "\n",
    "# apply best weights\n",
    "w_best  = best[\"w\"]\n",
    "oof_ens = np.sum(oof_mat.T  * w_best, axis=1)\n",
    "pred_ens= np.sum(pred_mat.T * w_best, axis=1)\n",
    "\n",
    "print(f\"[Ensemble] OOF RMSE: {rmse(y_np, oof_ens):.5f}\")\n",
    "\n",
    "# optional soft-clip\n",
    "def soft_clip_to_iqr(pred, y_train, verbose=True):\n",
    "    q1, q3 = np.percentile(y_train, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lo = q1 - 1.5 * iqr\n",
    "    hi = q3 + 1.5 * iqr\n",
    "    if verbose:\n",
    "        print(f\"[Clip-range] lo={lo:.3f}, hi={hi:.3f}\")\n",
    "    return np.clip(pred, lo, hi)\n",
    "\n",
    "pred_final = soft_clip_to_iqr(pred_ens, y_np, verbose=True) if SOFT_CLIP else pred_ens\n",
    "\n",
    "# save & download\n",
    "sub_out = sub.copy(); sub_out[\"y\"] = pred_final\n",
    "out_path = os.path.join(SAVE_DIR, \"submission_YJ_Estep.csv\")\n",
    "sub_out.to_csv(out_path, index=False)\n",
    "print(f\"✅ Saved -> {out_path}\")\n",
    "\n",
    "files.download(out_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
